# *CS3345.501 Sorting Algorithm Analysis Application*


## Intro

Welcome to this Sorting Algorithm Analysis Application! This README will go over the various assumptions I made while putting together this application, as well as include analysis of the sorting algorithms performance.  
This includes the way I handled list creation, counting comparisons/movements, the winning algorithms, the reasons why CPU time doesn't factor in, and additional analysis of the sorting algorithms across the 4 list types.

## Creating Lists

All of the lists are created with for loops, and an additional for loop is used to create an Integer type list for use with the Heap class and sorting algorithm.  
The random list is made by casting numbers generated by Math.random into integers. Inorder and Reverseorder lists are created using the for loop iterators, and Almostorder is created by randomizing the last 10% and the first 10% of the list, and creating the rest of the list in order.

Originally, for expediencies sake I chose to randomize only the last 20% of the Almostorder list, but after realizing I had run out of time to finish a GUI, I wanted to experiment with a different setup to catch recursive sorting algorithms differently (with both the first and second halves of the list now having at least some randomization). The results will be discussed in later analysis.

## How to count the comparisons/movements

For all sorts in general, if there is an if statement or a comparison made inside of a for loop(that isn't the for loop iterator), the comparisons are incremented.

Movements are counted as movements of elements in general, which includes swaps. Swaps are counted as a single movement.

Unique cases regarding comparisons/movements are that radix sort does not compare in it's sort, and for inorder lists insertion sort will never move elements.

## Why doesn't CPU Time factor into the Winning Algorithm?

It's because of the time's inconsistency.

*"But Ben, time is the only constant in life!"*

Well unfortunately, it's not a constant in terms of running time. For each time that I ran a sorting algorithm on any data type, the time varied by thousands of nano-seconds, and in the worst case, by >10 milliseconds. Trying to fit time into the Winning Algorithm determination in a meaningful way wasn't going to happen, especially since the sum of comparisons and movements already correlated to running time, as well as were more consistent.

Even though it doesn't directly contribute to the winning algorithm, taking it at face value can rule out algorithms that take hundreds or even a few thousand milliseconds to sort the list at the chosen list size of 50,000.

## The Winning algorithms

All of the data tables can be found in the Excel workbook bundled with the project. The list size of 50,000 was arbitrarily chosen, big enough to measure time in milliseconds as well as have differences in operation numbers large enough to easily see. The winning algorithm is chosen by summing the comparisons and movements, and then finding the algorithm with the least sum of operations performed. I wasn't able to find a way to effectively measure CPU memory usage either, and time isn't used per the section above.

### Random - Radix Sort

Radix sort wins the Random list type. Radix's lack of comparisons and constant number of movements for any size of list give it a consistency that outperforms any of sorting algorithm.

### InOrder - Insertion Sort

Insertion sort wins the inorder list type. Insertion sort doesn't have to move anything, and it's comparisons end up less than Radix sort's constant movement count of n times the number of digits in n.

### ReverseOrder - Quick Sort

Quick sort wins the reverseorder list type. In what is the worst case scenario for most other algorithms, Quick sort is able to keep both operation count and running time low enough to be Radix sort's consistency.

### AlmostOrder - Radix Sort

Radix Sorts consistency wins out once again. Interestingly enough, with the previous version of almostorder creation, Quick sort won, but after making it a little bit more random Quick sort was forced to make more comparisons and movements, over 50,000 (which at this point, is n). Heap sort was the only algorithm to improve efficiency, while Radix and Merge sort remained the same. All other algorithms decreased in efficiency with the new almostorder model.

## The Algorithms as a whole

Radix sort remains the most consistent algorithm, and can be considered the overall "winning" algorithm as a whole. It's lack of comparisons and constant movement size regardless of randomization makes it the jack-of-all-trades of the algorithms, and commonly the most efficient one.

Insertion and Selection sort are both very niche, bordering on not recommended for use. Insertion sort can go through sorted input faster than any other algorithm, but at that point, why are you using a sorting algorithm?  Selection sort on the other hand requires using the max amount of comparisons regardless of data type, which is often more than the sum of operations for most other algorithms.

Merge and Quick sort are consistently fast algorithms, not going above 10 milliseconds in running time regardless of list type. Quick sort comes in just ahead of Merge sort due to a smaller sum of operations that is a magnitude of 10 smaller. 

Heap sorts fall into the middle of the pack as algorithms go from this group. Heap sort regularly hits ~40-50 seconds of running time, and this is backed up by a sum of operations commonly lying in the middle between the absolute worst (Insertion/Selection sort) and the very best (Radix/Merge/Quick sort).
